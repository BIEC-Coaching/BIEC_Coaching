{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden von böntigeten Softwarebibliotheken und Festlegung von Grundeinstellungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Befehl ***import Name*** wird eine Bibliothek geladen.<br>\n",
    "Mit dem Befehl ***import Name as*** Abkürzung> wird eine Bibliothek geladen und kann mit der Abkürzung im weiteren Code genutzt werden.<br>\n",
    "Beispiel: ***import numpy as np*** lädt die Bibliothek numpy mit der Abkürzung np. Somit können wir im folgenden Funktionen der Bibliothek numpy mit ***np.Funktionsname*** aufrufen.<br><br>\n",
    "Überblick der genutzen Bibliotheken:<br>\n",
    "**numpy**: Effizient implementierte Funktionen für numerische Berechnungen von Vektoren und Matrizen.<br>\n",
    "**pandas**: Hilfsmittel für die Verwaltung von Daten und deren Analyse. Enthält insbesondere Datenstrukturen, sogenannte \"dataframes (kurz df)\", und Operatoren für den Zugriff auf numerische Tabellen und Zeitreihen.<br>\n",
    "**pandas_profiling**: Ermöglicht eine statistische Analyse der Datenstrukturen von pandas mit einem Befehl.<br>\n",
    "**matplotlib**: Ermöglicht mathematische Darstellungen aller Art anzufertigen.<br>\n",
    "**matplotlib.pyplot**: Enthält den Teil der genutzen Funktionen. Macht das künftige Aufrufen von Funktionen einfacher. (Statt ***mpl.pyplot.Funktionsname*** kann ***plt.Funktionsname*** geschrieben werden.)<br>\n",
    "**seaborn**: Baut auf der Bibliothek Matplotlib auf und erweitert diese um weitere Diagrammtypen, Maps und Plots.<br> \n",
    "**time**: Diese Bibliothek bietet verschiedene zeitbezogene Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren von Bibliotheken die benötigt werden\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Definiere Farben für Schaubilder\n",
    "fh_teal = '#179c7d'\n",
    "fh_orange = '#f29400'\n",
    "fh_blue = '#1f82c0'\n",
    "fh_red = '#e2001a'\n",
    "fh_lightgreen = '#b1c800'\n",
    "fh_beige = '#feefd6'\n",
    "fh_grey = '#e1e3e3'\n",
    "\n",
    "# Globale Einstellung für Schriftgröße, Farben usw. in Schaubildern\n",
    "fh_palette = [fh_teal, fh_orange, fh_blue, fh_red, fh_lightgreen, fh_beige, fh_grey]\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '.9'})\n",
    "sns.set_palette(fh_palette)\n",
    "params = {'legend.fontsize': 22,\n",
    "          'figure.figsize': (20, 14),\n",
    "          'axes.labelsize': 22,\n",
    "          'axes.titlesize': 26,\n",
    "          'xtick.labelsize': 20,\n",
    "          'ytick.labelsize': 20}\n",
    "mpl.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initiale Exploration der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sehen die Trainingsdaten aus:<br>\n",
    "<img src=\"./Bilder/train_FD001.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der readme Datei wissen wir wie die Daten aufgebaut sind:\n",
    "<img src=\"./Bilder/readme.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Daten:\n",
    "Als nächstes werden die Trainingsdaten geladen. Zuerst legen wir jedoch die Namen der einzelnen Merkmale (Spalten) fest. Diese können wir der Readme entnehmen:<br>\n",
    "1)\tunit number<br>\n",
    "2)\ttime, in cycles<br>\n",
    "3)\toperational setting 1<br>\n",
    "4)\toperational setting 2<br>\n",
    "5)\toperational setting 3<br>\n",
    "6)\tsensor measurement  1<br>\n",
    "7)\tsensor measurement  2<br>\n",
    "...<br>\n",
    "26)\tsensor measurement  26<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Festlegung der Namen der einzenlnen Merkmale (Spalten)\n",
    "# Wir erstellen eine Lsite mit den ersten beiden Namen\n",
    "index = ['engine#','tCycles']\n",
    "\n",
    "# Wir erstellen eine Lsite mit den Namen für die Settings\n",
    "settings = ['setting1','setting2','setting3']\n",
    "\n",
    "# In der dritten Zeile erstellen wir eine Lsite mit den Namen für die Sensoren. Hierbei nutzen wir eine kleine Funktion,\n",
    "# die eine Liste mit den Namen sensor1 bis sensor 24 erstellt.\n",
    "sensors = ['sensor' + str(i) for i in range(1,24)]\n",
    "\n",
    "# wir erstellen eine Liste col_names, die alle Namen enthält. Dabei können wir die vorherigen listen einfach mit\n",
    "# einem + Verknüpfen\n",
    "col_names = index + settings + sensors\n",
    "\n",
    "# Mit dem Aufrufen von col_names können wir uns die Liste anschauen\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Laden der Trainingsdaten mit dem Befehl pd.read_csv(...)\n",
    "# Der Funktion teilen wir den Pfad und Dateinamen mit, mit sep=' ' teilen wir der Funktion mit, dass die einzelnen\n",
    "# Werte in einer Zeile durch ein Leerzeichen gtrennt sind und mit names=col_names teilen wir die Namen der Spalten mit.\n",
    "df_train = pd.read_csv('./CMAPSSData/train_FD001.txt', sep=' ', names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Aufgabe 1\n",
    "Nicht alle Sensoren liefern sinnvolle Informationen. Welche Sensoren sind das und sollten von der weiteren Analyse ausgeschlossen werden?\n",
    "Die Daten und Sensoren können in den nächsten Zellen genauer betrachtet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ein Blick auf die Trainingsdaten. Jede Zeile ist eine Beobachtung, jede Spalte ein Merkmal.\n",
    "# Der Wert NaN (Abkürzung für: not a number) bedeutet, dass an dieser Stelle der Wert fehlt. \n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellung einer Funktion, mit der in einem Schaubild des Sensorverlaufs von mehreren Engines dargestellt werden kann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung der Funktion Namens plot_sensor, die als Eingabe die Variable sensor_name benötigt.\n",
    "def plot_feature(feature_name):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    for i in df_train['engine#'].unique():\n",
    "        if (i % 20 == 0):\n",
    "            plt.plot(np.arange(0, df_train[df_train['engine#']==i].shape[0]),\n",
    "                     feature_name,\n",
    "                     data=df_train[df_train['engine#']==i])\n",
    "    plt.xlim(0, 250)\n",
    "    plt.xticks(np.arange(0, 275, 25))\n",
    "    plt.ylabel(feature_name)\n",
    "    plt.xlabel('\"Zeit\" angegeben in #tCycles')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der nächsten Zelle kann noch einmal der Verlauf von verschiedenen Merkmale dargestellt werden. Hierfür für wird die gerade erstellte Funktion <font color='blue'>plot_feature</font> genutzt. Durch die eingabe des Merkmalsnamen, z.B. <font color='darkred'>engine#</font> wird das Merkmal engine# für fünf verschiedene Engines über die Zeit (in tCycles) dargestellt. Man kann durch <font color='darkred'>engine#</font> durch den Namen eines anderen Merkmals ersetzen (Spalten in der Tabelle oberhalb). Die <font color='darkred'>' '</font> müssen stehen bleiben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines Schaubilds\n",
    "plot_feature('engine#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wollen wir uns einen Überblick über den Datensatz machen. Dabei hilft uns die Bibliothek pandas_profiling, die nur die eine Funktion \"profile_report\" beinhaltet. Durch den profile_report können wir mit Hilfe eines Befehls einen detallirte Übersicht über den Datnsatz erhalten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines Profile Reports, der einen Überblick über den Datensatz und die Merkmale gibt.\n",
    "# correlations=None verhindert, dass Korrelationen zwischen den Merkmalen berechnet werden. Dies wird hier hauptsächlich\n",
    "# aus Performance Gründen gemacht, da die Berechnung der Korrelationen einige Zeit in Anspruch nimmt.\n",
    "# Bei Interesse kann correlations=None gelöscht werden: pandas_profiling.ProfileReport(df_train)\n",
    "profile_report = pandas_profiling.ProfileReport(df_train, correlations=None)\n",
    "profile_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ende Aufgabe 1\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der nächsten Zelle werden alle Merkmale, die keine Informationen oder Mehrwert liefern, gelöscht. Dabei werden alle Merkmale (Spalten) gelöscht, die in der List ***drop_cols*** stehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen von Merkmalen (Spalten), die keine Information beinhalten\n",
    "# Die Funktion drop(...) können Zeilen oder Spalten gelöscht werden, dabei wird der Funktion folgende Paramter mitgegeben:\n",
    "# labels: Liste von Zeilen- oder Spaltennamen, die gelöscht werden sollen\n",
    "# axis: Gibt an, ob Zeilen oder Spalten gelöscht werden sollen, 0=Zeilen, 1=Spalten\n",
    "drop_cols = ['setting3', 'sensor1', 'sensor5', 'sensor6', 'sensor10',\n",
    "             'sensor16', 'sensor18', 'sensor19', 'sensor22', 'sensor23']\n",
    "df_train = df_train.drop(labels=drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier laden wir nun noch die Testdaten. Die Funktion für das Einlesen der Daten ist gleich wie bei den Trainingsdaten und da die Testdaten gleich aufgebaut sind wie die Trainingsdaten können wir die gleichen Spaltennamen nutzen. <br>\n",
    "Anschließend löschen wir direkt die gleichen Spalten, die wir auch im Trainingsdatensatz gelöscht haben. Auch hier ist die Funktion dieselbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Testdaten\n",
    "df_test = pd.read_csv('./CMAPSSData/test_FD001.txt', sep=' ', names=col_names)\n",
    "\n",
    "# Löschen der selben Splaten, die auch im Trainingsdatensatz gelöscht wurden.\n",
    "df_test = df_test.drop(drop_cols,axis=1)\n",
    "\n",
    "# Anzeigen der Testdaten\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering / Merkmale konstruieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Trainieren von Algorithmen benötigen wir eine Zielvariable. Die Zielvariable, ist das Merkmal, das wir in Zukunft mit Hilfe unseres KI-Algorithmus vorhersagen wollen. Eine solche Zielvariable müssen wir im vorliegendan Fall erst noch festlegen und ggf. erst noch konstruieren. Dabei gibt es kein richtig oder falsch, sondern mehrere Möglichkeiten dies zu tun. An dieser Stelle ist im Allgemeinen ein intensiver Austausch mit Domänenexperten sehr hilfreich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Aufgabe 2\n",
    "Überlegen Sie sich was wir im vorliegenden Fall vorhersagen wollen und welches die Zielvariable sein könnte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellung der Restlaufdauer (Remaining Useful Lifetime, RUL)\n",
    "Eine Möglichkeit für die Zielvariable ist diesogenannte Remaining Useful Lifetime (RUL). Diese gibt für jeden Zeile an, wie lange die Maschine noch läuft, bis ein Defekt auftritt. Die RUL haben wir nicht direkt in den Date vorliegen und wir müssen diese erst aus der Laufzeit tCycle konstruieren.<br>\n",
    "Die vermutlich einfachtse und naheliegendste Möglichkeit, die RUl zu definieren ist die Zeit vom Defekt rückwärts uz zählen. D.h. im letzen Durchlauf (tCycle) einer Engine wird die RUL auf 0 gesetzt, da kein weiterer Durchlauf mehr möglich ist. Dann wird rückwärts hochgezählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Funktion \"add_remaining_useful_life\", die die RUL für jede Engine erzeugt.\n",
    "def add_remaining_useful_life(df):\n",
    "    # Erhalte die maximale Anzahl von Durchläufen pro Triebwerk (engine)\n",
    "    grouped_by_unit = df_train.groupby(by='engine#')\n",
    "    max_cycle = grouped_by_unit['tCycles'].max()\n",
    "    \n",
    "    # Erstelle eine neue Spalte \"max_cycle\", die die maximale Anzahl an Zyklus für jedes Triebwerk beinmhaltet\n",
    "    result_frame = df_train.merge(max_cycle.to_frame(name='max_cycle'), left_on='engine#', right_index=True)\n",
    "    \n",
    "    # Berechne die RUL für jede Beobachtung (Zeile)\n",
    "    remaining_useful_life = result_frame['max_cycle'] - result_frame['tCycles']\n",
    "    result_frame['RUL'] = remaining_useful_life\n",
    "    \n",
    "    # Lösche die Spalte \"max_cycle\", da sie nicht Länger benötigt wird\n",
    "    result_frame = result_frame.drop('max_cycle', axis=1)\n",
    "    return result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne die RUL für den Trainingsdatensatz\n",
    "df_train = add_remaining_useful_life(df_train)\n",
    "\n",
    "# Zeige die Testdatenmatrix mit dem neuen Merkmal \"RUL\"\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstelln von Trainings- und Testdatensatz\n",
    "Die Testdaten und die Trainingsdaten werden jeweils in zwei Teile Aufgeteilt. Dabei ist X_train bzw. X_test immer der Datensatz, den der Algorithmus als Input bekommt und y_train bzw. y_test ist das Label, welches vom Algorithmus vorausgesagt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Erstelln von Trainings- und Testdatensatz\n",
    "X_train = df_train.copy()\n",
    "y_train = X_train.pop('RUL')\n",
    "\n",
    "\n",
    "X_test = df_test.copy()\n",
    "X_test = df_test.groupby('engine#').last().reset_index()\n",
    "\n",
    "y_test = pd.read_csv(('./CMAPSSData/RUL_FD001.txt'), sep='\\s+', header=None, names=['RUL'])\n",
    "\n",
    "X_train = X_train.drop(labels=['tCycles'], axis=1)\n",
    "X_test = X_test.drop(labels=['tCycles'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Modellierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "#### Importiere benötigte Bibliotheken und Funktionen für die Modellierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as tfk\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.compat.v1.logging import set_verbosity\n",
    "from tensorflow.compat.v1.logging import ERROR\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Verhindere Warnungen, die durch die mybinder Umgebung erzeugt werden und ignoriert werden können\n",
    "set_verbosity(ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore', message ='Tight layout not applied.');\n",
    "\n",
    "# Setze einen Seed für den Zufallsnummergenerator für Reproduzierbarkeit\n",
    "np.random.seed(1)\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisierung (Skalieren) der Daten\n",
    "Die Standardisierung der Daten wird für einige Algorithmen vorausgesetzt. Dabei werden alle Merkmale in einen vergleichbaren Wertebereich überführt. Da viele Algorithmen mit Zahlen arbeiten und diese mathematischen Operationen unterziehen besteht ansonsten das Problem, dass große Zahlen einen großen Einfluss auf den Algorithmus haben und z.B. die Wahl zwischen Metern und Millimetern einen großen Unterschied spielen würde, obwohl die Bedeutung von 1 m und 1000 mm die selbe ist.<br>\n",
    "Es verschiedne Möglichkeiten die Merkmale zu Transformieren. Die wichtigsten sind:\n",
    "* die Standardisierung (z-Transformation) bei der jedes Merkmal so trasformiert wird, dass es einen Mittelwert von 0 und eine Standaerabweichung von 1 hat. Dadurch bleiben die relativen Abstände innerhalb eines Merkmals erhalten.<br>\n",
    "* die Normalisierung bei der alles Werte in einen Bereich zwischen 0 und 1 überführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalieren der Daten mit Hilfe der Funktion StandardScaler\n",
    "\n",
    "# Definition des Skalierers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Anwenden des Skalierers auf die Trainingsdaten\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Anwenden des Skalierers auf die Testdaten\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Definition einer Loss Function\n",
    "Die Loss Function wird genutzt um den Fehler (loss) des Algorithmus zu berechnen. Anhnd des Loss lässt sich der ALgorithmus optimieren (möglichst kleiner Fehler) und es können anhand der Größe des Fehlers unterschiedliche Algorithmen verglichen werden. Es gibt viele verschiedene Loss Functions, die genutzt werden können. Einige wichtige Loss Functions sind:\n",
    "* 0-1 loss\n",
    "* RMSE (root mean squared error)\n",
    "* Likelihood loss\n",
    "* Hinge loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Definition des RMSE, das im Folgenden als Loss genutzt wird\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tfk.backend.sqrt(tfk.backend.mean(tfk.backend.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zum Start ein einfacher Algorithmus / Modell: Lineare Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition der Begriffe Algorithmus und Modell\n",
    "* Ein Algorithmus ist eine eindeutige Handlungsvorschrift zur Lösung eines Problems oder einer Klasse von Problemen. Algorithmen bestehen aus endlich vielen, wohldefinierten Einzelschritten. Damit können sie zur Ausführung in ein Computerprogramm implementiert, aber auch in menschlicher Sprache formuliert werden. Bei der Problemlösung wird eine bestimmte Eingabe in eine bestimmte Ausgabe überführt. [1]\n",
    "* Ein Maschine Learning (ML) Algorithmus ist ein Algorithmus aus dem Bereich des Machine Learnings. Dieser Algorithmus wird mit Daten trainiert und lernt aus diesen Zusammenhänge und Muster. Ein Algorithmus ist somit immer in der Entwicklungsphase und noch Gegenstand von Anpassung und Optimierung. Für die optimierung eines Modells wird viel Rechenleistung benötigt.\n",
    "* Ein (ML) Modell ist ein mit den vorhandenen Daten fertig trainierter und optimierter Algorithmus. Das Modell ist in der Lage für neue Daten, die dem Algorithmus während des Trainings unbekannt waren, Aussagen zu treffen. Ein ML-Modell kann in einen Produktiveinsatz gebracht werden und in bestehenden IT-Systeme integriert werden. Im produktiveinsatz benötigt ein Modell normalerweise nur geringe Rechenleistung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition des Algorithmus\n",
    "model_linear = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des Algorithmus\n",
    "model_linear.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definition einer Funktion, die den Trainings- und Testfehler berechnet und ausgibt\n",
    "def evaluate(y_true, y_hat, label='test'):\n",
    "    mse = mean_squared_error(y_true, y_hat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    variance = r2_score(y_true, y_hat)\n",
    "    print('{} set RMSE:{}, R2:{}'.format(label, rmse, variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des Modells\n",
    "y_hat_train = model_linear.predict(X_train_scaled)\n",
    "evaluate(y_train, y_hat_train, 'train')\n",
    "\n",
    "y_hat_test = model_linear.predict(X_test_scaled)\n",
    "evaluate(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Aufgabe 3\n",
    "Der Fehler für das Trainingsset ist \"train set RMSE\", der Fehler des Testsets ist \"test set RMSE\". Es fällt auf, dass der Testfehler kleiner ist als der Trainingsfehler. Im Allgemeinen sollte dies genau umgekehrt sein.\n",
    "<br>\n",
    "<br>\n",
    "Versuchen Sie folgende Fragen zu beantworten:\n",
    "<br>\n",
    "Warum ist der Trainingsfehler normalerweise kleiner als der Testfehler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ende Aufgabe 3\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genaue Betrachtung der RUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Aufgabe 4\n",
    "Genaue Betrachtung der Restnutzungsdauer (RUL), die wie vorhin selbst erstellt haben.\n",
    "<br>\n",
    "Was fällt bei dieser auf?\n",
    "<br>\n",
    "Worin könnten Probleme liegen (Trainingsdaten vs. Testdaten)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Erstellen zweier Schaubilder. Diese zeigen die Verteilung der RUL im Trainings- und im Testdatensatz\n",
    "\n",
    "# Berechnung der maximalen RUL jeder Engines bzw. wie viele Zeitschritte eine Engine funktioniert bis zum Defekt.\n",
    "df_max_rul = df_train[['engine#', 'RUL']].groupby('engine#').max().reset_index()\n",
    "\n",
    "# Estellung eines \"Canvas\", eines leeren Bildes\n",
    "fig = plt.figure(tight_layout=True)\n",
    "\n",
    "# Hinzufügen des ersten Plots\n",
    "ax = fig.add_subplot(211)\n",
    "df_max_rul['RUL'].hist(ax=ax, bins=range(25, 350, 20))\n",
    "plt.xlabel('RUL')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.title('Trainingsdaten')\n",
    "\n",
    "# Hinzufügen des zweiten Plots\n",
    "ax2 = fig.add_subplot(212)\n",
    "y_test['RUL'].hist(ax=ax2, bins=range(25, 350, 20))\n",
    "plt.xlabel('RUL')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.title('Testdaten')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung eines Schabilds, welches die RUL und einen beispielhaften Sensor für ein Triebwerk zeigt\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(13,5))\n",
    "\n",
    "unit20_RUL = np.array(df_train.loc[df_train['engine#']==20, 'RUL'])\n",
    "unit20_sensor12 = np.array(df_train.loc[df_train['engine#']==20, 'sensor12'])\n",
    "\n",
    "signal = ax1.plot(unit20_RUL, unit20_sensor12, color=fh_blue)\n",
    "\n",
    "plt.xlim(250, 0)\n",
    "plt.xticks(np.arange(0, 275, 25))\n",
    "ax1.set_ylabel('Sensor 12', labelpad=20)\n",
    "ax1.set_xlabel('RUL', labelpad=20)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "rul_line = ax2.plot(unit20_RUL, unit20_RUL, 'k', linewidth=4)\n",
    "ax2.set_ylabel('RUL', labelpad=20)\n",
    "\n",
    "ax2.set_ylim(0, 250)\n",
    "ax2.set_yticks(\n",
    "    np.linspace(ax2.get_ybound()[0], ax2.get_ybound()[1], 6))\n",
    "ax1.set_yticks(\n",
    "    np.linspace(ax1.get_ybound()[0], ax1.get_ybound()[1], 6))\n",
    "\n",
    "lines = signal+rul_line\n",
    "labels = ['sensor12', 'RUL']\n",
    "ax1.legend(lines, labels, loc=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ende Aufgabe 3\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellung einer besseren / optimierten Restlaufdauer\n",
    "Für die optimierte RUL werden alle Werte oberhalb von 125 auf 125 gesetzt. Ob dies wirklich sinnvoll ist, sollte an dieser Stelle mit Domain Experten diskutiert werden und auch der Wert 125 evaluiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der optimierten Restlaufdauer RUL \n",
    "y_train_optimized = y_train.clip(upper=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Erstellung eines Schabilds, welches die RUL, die optimierte RUL und einen beispielhaften Sensor für ein Triebwerk zeigt\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(13,5))\n",
    "\n",
    "signal = ax1.plot(unit20_RUL, unit20_sensor12, color=fh_blue)\n",
    "\n",
    "rul = df_train.loc[df_train['engine#']==20, 'RUL']\n",
    "unit20_RUL_optimized = unit20_RUL.copy()\n",
    "unit20_RUL_optimized[unit20_RUL_optimized >= 125] = 125\n",
    "\n",
    "plt.xlim(250, 0)\n",
    "plt.xticks(np.arange(0, 275, 25))\n",
    "ax1.set_ylabel('Sensor 12', labelpad=20)\n",
    "ax1.set_xlabel('verbleibende Laufzeit', labelpad=20)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "rul_line = ax2.plot(unit20_RUL, unit20_RUL, 'k', linewidth=4)\n",
    "\n",
    "rul_line2 = ax2.plot(unit20_RUL, unit20_RUL_optimized, '--', linewidth=4, color=fh_teal)\n",
    "\n",
    "ax2.set_ylim(0, 250)\n",
    "ax2.set_yticks(\n",
    "    np.linspace(ax2.get_ybound()[0], ax2.get_ybound()[1], 6))\n",
    "ax1.set_yticks(\n",
    "    np.linspace(ax1.get_ybound()[0], ax1.get_ybound()[1], 6))\n",
    "\n",
    "lines = signal+rul_line+rul_line2\n",
    "labels = ['Sensor 12', 'RUL', 'Optimierte RUL']\n",
    "ax1.legend(lines, labels, loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineare Regression mit optimiertem RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Model Training\n",
    "lm.fit(X_train_scaled, y_train_optimized)\n",
    "\n",
    "# Model Evaluation\n",
    "y_hat_train = lm.predict(X_train_scaled)\n",
    "evaluate(y_train_optimized, y_hat_train, 'train')\n",
    "y_hat_test = lm.predict(X_test_scaled)\n",
    "evaluate(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitere Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Definition eines Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_forest = RandomForestRegressor(n_estimators=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Definition eines Neural Network (Input Layer und Output Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optim = tfk.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "inputs = tfk.Input(shape=(17,))\n",
    "outputs = tfk.layers.Dense(1, activation=tfk.activations.relu)(inputs)\n",
    "\n",
    "model_nn = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "model_nn.compile(optimizer='rmsprop',loss = root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Definition eines Tiefen Neuronalen Netzes (Deep Neural Network; Input Layer, 3 Hidden Layer, Output Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = tfk.Input(shape=(17,))\n",
    "x = tfk.layers.Dense(17, \n",
    "                     activation=tfk.activations.relu,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     activity_regularizer=tfk.regularizers.l1(0.00002))(inputs)\n",
    "x = tfk.layers.Dense(70, \n",
    "                     activation=tfk.activations.relu,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     activity_regularizer=tfk.regularizers.l1(0.00002))(x)\n",
    "x = tfk.layers.Dense(60, \n",
    "                     activation=tfk.activations.relu,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     activity_regularizer=tfk.regularizers.l1(0.00002))(x)\n",
    "x = tfk.layers.Dense(50, \n",
    "                     activation=tfk.activations.relu,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     activity_regularizer=tfk.regularizers.l1(0.00002))(x)\n",
    "x = tfk.layers.Dense(40, \n",
    "                     activation=tfk.activations.relu,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     activity_regularizer=tfk.regularizers.l1(0.00002))(x)\n",
    "x = tfk.layers.Dense(10,\n",
    "                     activation=tfk.activations.relu,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     activity_regularizer=tfk.regularizers.l1(0.00002))(x)\n",
    "outputs = tfk.layers.Dense(1,\n",
    "                           activation=tfk.activations.relu,\n",
    "                          activity_regularizer=tfk.regularizers.l1(0.00002))(x)\n",
    "\n",
    "model_deep_nn = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "model_deep_nn.compile(optimizer=optim,\n",
    "                      loss = root_mean_squared_error,\n",
    "                     metrics=[root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Definition einer Support Vector Maschine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supVecMac = svm.SVR(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training des Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_forest.fit(X_train_scaled, y_train_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training des Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandeln der Daten in ein Datenformat, mitdem das Neuronale Netz arbeiten kann.\n",
    "X_train_scaled = np.float32(X_train_scaled)\n",
    "y_train_optimized = np.float32(y_train_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_nn.fit(X_train_scaled, y_train_optimized, batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training des Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_deep_nn.fit(X_train_scaled, y_train_optimized, batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training der Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supVecMac.fit(X_train_scaled, y_train_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Evaluation & vergleich aller Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation der zueben Trainierten Modelle\n",
    "model_linear = linear_model.LinearRegression()\n",
    "model_linear.fit(X_train_scaled, y_train_optimized)\n",
    "\n",
    "models = [model_linear, model_forest, model_nn, model_deep_nn, supVecMac]\n",
    "\n",
    "metric = metrics.mean_squared_error\n",
    "metric_per_model = []\n",
    "for model in models:\n",
    "    prediction = model.predict(X_test_scaled)\n",
    "    cost = metric(y_test, prediction)\n",
    "    cost = np.sqrt(cost)\n",
    "    metric_per_model.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Darstellung der Testfehler in einem Schaubild\n",
    "x = np.arange(len(metric_per_model))\n",
    "model_names = ['Lineare \\n Regression', 'Random Forest', 'Neural \\n Network', 'Deep Neural \\n Network', 'Support \\n Vector Machine']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.bar(x, metric_per_model, .5)\n",
    "plt.xticks(x,[model for model in model_names])\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., .2 + height,\n",
    "            '%8.2f' % height,\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=20)\n",
    "\n",
    "plt.title('Testfehler verschiedener Algorithmen')\n",
    "plt.ylabel('Testfehler (RMSE)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "# Hyperparameter-Optimierung (Hyperparamter tunining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Aufgabe 5\n",
    "Welches sind die optimaten Hyperparamter für einen Random Forest?\n",
    "<br>\n",
    "Versuche verschiedene Hyperparametr aus. Dabei können in der nächsten Zelle die folgenden Hyperparameter angepasst werden:\n",
    "<br>\n",
    "<b>n_estimator:</b> Anzahl der Bäume (ganze Zahl zwischen 2 und 100. Je höher die Zahl, desto länger dauert die Berechnung)\n",
    "<br>\n",
    "<b>max_depth:</b> Maximale Tiefe eines Baumes (ganze Zahl zwischen 2 und 15 oder None, falls keine maximale Tiefe vorgegebn werden soll)\n",
    "<br>\n",
    "<b>max_features:</b> Maximale Anzhal der Merkmale, die für das erstellen eines Baumes zufällig ausgewählt wertden (ganze Zahl zwischen 2 und 15 oder “auto”, “sqrt”, “log2”. Der Standardwert ist sqrt(n_features), die Quadratwurzel der Anzahl der Merkmale)\n",
    "<br>\n",
    "Es gibt noch viele weitere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition des Random Forests\n",
    "model_forest2 = RandomForestRegressor(n_estimators=100, max_depth=5, max_features='sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des Random Forests\n",
    "model_forest2.fit(X_train_scaled, y_train_optimized)\n",
    "y_pred = model_forest2.predict(X_train_scaled)\n",
    "print('Trainingsfehler (RMSE):', np.sqrt(metrics.mean_squared_error(y_train_optimized, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ende Aufgabe 5\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zufällige Suche (Random Search) der optimalen Hyperparamter für einen Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Random Search mit Angabe, welche Hyperparameter mit welchen werten getestet werden sollen\n",
    "def Random_Search_CV_RFR(X_train, y_train):\n",
    "\n",
    "    estimator = RandomForestRegressor()\n",
    "    param_grid = { \n",
    "            'n_estimators'      : [10, 50, 100],\n",
    "            'max_features'      : ['sqrt', 'log2'],\n",
    "            'max_depth'         : [2, 4, 6, 8, 10, 12, 14, None],\n",
    "            'min_samples_split' : [2,4,8,16],\n",
    "            'bootstrap': [True, False]\n",
    "            }\n",
    "\n",
    "    random_params = RandomizedSearchCV(estimator, param_grid, n_iter=50, n_jobs=-1, cv=5)\n",
    "\n",
    "    random_params.fit(X_train, y_train)\n",
    "\n",
    "    return random_params.best_score_ , random_params.best_params_\n",
    "\n",
    "def RFR(X_train, X_test, y_train, y_test, best_params):\n",
    "    \n",
    "    estimator = RandomForestRegressor(n_jobs=-1).set_params(**best_params)\n",
    "    estimator.fit(X_train,y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    return y_test,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchführen der Random Search für den Random Forest\n",
    "best_score, best_params = Random_Search_CV_RFR(X_train_scaled, y_train_optimized)\n",
    "y_test , y_predict = RFR(X_train_scaled, X_test_scaled, y_train_optimized, y_test, best_params)\n",
    "print('Best params:', best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Random Forest mit den optimalen Hyperparametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Random Forest mit den optimalen Hyperparametern wird nun noch einmal mit allen Daten trainiert\n",
    "model_forest3 = RandomForestRegressor(n_estimators=100, min_samples_split=8, max_features='sqrt',\n",
    "                                       max_depth=12, bootstrap=True)\n",
    "model_forest3.fit(X_train_scaled, y_train_optimized)\n",
    "y_pred = model_forest3.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testfehler des Random Forest mit den optimalen Hyperparametern\n",
    "y_pred = model_forest3.predict(X_test_scaled)\n",
    "print('Testfehler (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich aller Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation der Trainierten Modelle, inklusive des optimierten Random Forest\n",
    "models2 = [model_linear, model_forest, model_forest3, model_nn, model_deep_nn, supVecMac]\n",
    "\n",
    "metric = metrics.mean_squared_error\n",
    "metric_per_model2 = []\n",
    "for model in models2:\n",
    "    prediction = model.predict(X_test_scaled)\n",
    "    cost = metric(y_test, prediction)\n",
    "    cost = np.sqrt(cost)\n",
    "    metric_per_model2.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Darstellung der Testfehler in einem Schaubild\n",
    "x = np.arange(len(metric_per_model2))\n",
    "model_names2 = ['Lineare \\n Regression', 'Random Forest', 'Optimierter  \\n Random Forest', 'Neural \\n Network', 'Deep Neural \\n Network', 'Support \\n Vector Machine']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.bar(x, metric_per_model2, .5)\n",
    "plt.xticks(x,[model for model in model_names2])\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., .2 + height,\n",
    "            '%8.2f' % height,\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=20)\n",
    "\n",
    "plt.title('Testfehler verschiedener Algorithmen')\n",
    "plt.ylabel('Testfehler (RMSE)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
